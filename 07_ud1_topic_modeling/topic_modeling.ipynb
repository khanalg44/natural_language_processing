{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection.\n",
    "\n",
    "As the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making. \n",
    "\n",
    "Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts.\n",
    "\n",
    "A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "Topic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example – New York Times are using topic models to boost their user – article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.\n",
    "\n",
    "There are many approaches for obtaining topics from a text such as – Term Frequency and Inverse Document Frequency (TfIdf). NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation(LDA) is the most popular topic modeling technique and in this article, we will discuss the same.\n",
    "\n",
    "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, re\n",
    "import nltk\n",
    "\n",
    "utils_dir = '/Users/gshyam/utils/'\n",
    "sys.path.append(utils_dir)\n",
    "\n",
    "from nlp_utils import prepare_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how it works with the following sentences.\n",
    "\n",
    "doc1 = \"I have big exam tomorrow and I need to study hard to get a good grade. this Exam is hard.\"\n",
    "doc2 = \"My wife likes to go out with me but I prefer staying at home and studying.\"\n",
    "doc3 = \"Kids are playing football in the field and they seem to have fun\"\n",
    "doc4 = \"Sometimes I feel depressed while driving and it's hard to focus on the road.\"\n",
    "doc5 = \"I usually prefer reading at home but my wife prefers watching a TV.\"\n",
    "\n",
    "# array of documents aka corpus\n",
    "corpus = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Tokenizing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['big',\n",
       "  'exam',\n",
       "  'tomorrow',\n",
       "  'need',\n",
       "  'study',\n",
       "  'hard',\n",
       "  'get',\n",
       "  'good',\n",
       "  'grade',\n",
       "  'exam',\n",
       "  'hard'],\n",
       " ['wife', 'likes', 'go', 'prefer', 'staying', 'home', 'studying'],\n",
       " ['kids', 'playing', 'football', 'field', 'seem', 'fun'],\n",
       " ['sometimes', 'feel', 'depressed', 'driving', 'hard', 'focus', 'road'],\n",
       " ['usually', 'prefer', 'reading', 'home', 'wife', 'prefers', 'watching', 'tv']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = [prepare_text(doc, TOKENIZE=True) for doc in corpus]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 items in the dictionary: key is index and value are the words\n",
      "(0, 'big')\n",
      "(1, 'exam')\n",
      "(2, 'get')\n",
      "(3, 'good')\n",
      "(4, 'grade')\n",
      "(5, 'hard')\n",
      "(6, 'need')\n",
      "(7, 'study')\n",
      "(8, 'tomorrow')\n",
      "(9, 'go')\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "\n",
    "print (\"First 10 items in the dictionary: key is index and value are the words\")\n",
    "for item in list(dictionary.items())[:10]:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) method \n",
    "this is a common and very popular method to convert a document in text form into numerical values which can be fed into a model. In this method each unque word in the doc is assignmed a label and the number of times a word appears in the doc is also assigned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "numerical_corpus = [dictionary.doc2bow(text) for text in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
       " [(16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)],\n",
       " [(5, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)],\n",
       " [(10, 1), (12, 1), (15, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the first line that second entry `(1,2)` represents word `exam` with index 1 and it appears 2 times in the doc. Similarly the word `hard` appears twice. hence we have `(5,2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model\n",
    "\n",
    "The LDA model discovers the different topics that the documents represent and how much of each topic is present in a document. \n",
    "\n",
    "Python provides many great libraries for text mining practices, “gensim” is one such clean and beautiful library to handle text data. It is scalable, robust and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0 : 0.030*\"hard\" + 0.030*\"wife\" + 0.030*\"field\" + 0.030*\"prefer\" + 0.030*\"fun\"\n",
      "Topic #1 : 0.064*\"wife\" + 0.064*\"focus\" + 0.064*\"driving\" + 0.064*\"depressed\" + 0.064*\"hard\"\n",
      "Topic #2 : 0.030*\"hard\" + 0.030*\"prefer\" + 0.030*\"wife\" + 0.030*\"kids\" + 0.030*\"fun\"\n",
      "Topic #3 : 0.030*\"home\" + 0.030*\"wife\" + 0.030*\"prefer\" + 0.030*\"hard\" + 0.030*\"sometimes\"\n",
      "Topic #4 : 0.030*\"hard\" + 0.030*\"prefer\" + 0.030*\"wife\" + 0.030*\"fun\" + 0.030*\"home\"\n",
      "Topic #5 : 0.030*\"hard\" + 0.030*\"wife\" + 0.030*\"field\" + 0.030*\"prefer\" + 0.030*\"sometimes\"\n",
      "Topic #6 : 0.147*\"exam\" + 0.147*\"hard\" + 0.077*\"grade\" + 0.077*\"big\" + 0.077*\"tomorrow\"\n",
      "Topic #7 : 0.030*\"home\" + 0.030*\"hard\" + 0.030*\"prefer\" + 0.030*\"wife\" + 0.030*\"field\"\n",
      "Topic #8 : 0.064*\"home\" + 0.064*\"prefer\" + 0.064*\"football\" + 0.064*\"usually\" + 0.064*\"watching\"\n",
      "Topic #9 : 0.030*\"hard\" + 0.030*\"wife\" + 0.030*\"prefer\" + 0.030*\"home\" + 0.030*\"kids\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "model = LdaModel(corpus=numerical_corpus, num_topics=10, id2word=dictionary)\n",
    "\n",
    "all_topics = model.print_topics()\n",
    "\n",
    "for i in range(10):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(f\"Topic #{i} : {model.print_topic(i, 5 )}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we trained and built our LDA model over the five simple sentences, whenever we want to detect the topic of a new sentence or text, we'll at first prepare the text and then push that into our model to get a topic. Let's try to predict a topic for a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "Let's find out a topic for a new doc using the previously trained model. \n",
    "`My wife plans to go out tonight.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wife', 'plans', 'go', 'tonight']\n",
      "[(9, 1), (15, 1)]\n"
     ]
    }
   ],
   "source": [
    "doc_new = \"My wife plans to go out tonight.\"\n",
    "doc_new_prepared = prepare_text(doc_new, TOKENIZE=True)\n",
    "print ( doc_new_prepared )\n",
    "doc_bow = dictionary.doc2bow(doc_new_prepared)\n",
    "print (doc_bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here since our dictionary is not large enough the bag of words for the new doc has missed a couple of words `plans` and `tonight`. As only the words `wife: index=15` and `go : index=9` exists in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orinal list :\t\t[(2, 1), (3, 4), (4, 1), (1, 3)] \n",
      "sort with first:\t[(4, 1), (3, 4), (2, 1), (1, 3)] \n",
      "Sorted with second:\t[(3, 4), (1, 3), (4, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "def sort_list(A, key=0):\n",
    "    # sort a list taking the take element of each item\n",
    "    A_sorted = sorted(A, key=lambda x: x[key] )\n",
    "    # reverse the sorted list to make the first element the largest\n",
    "    A_sorted.reverse()\n",
    "    return A_sorted\n",
    "\n",
    "A = [(2, 1), (3, 4), (4, 1), (1, 3)]\n",
    "A0=sort_list(A, 0)\n",
    "A1=sort_list(A, 1)\n",
    "print (f\"orinal list :\\t\\t{A} \\nsort with first:\\t{A0} \\nSorted with second:\\t{A1}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def print_topics(topics_sorted, all_topics, k=2):\n",
    "def print_top_k(topics, all_topics, k=2):\n",
    "    topics_sorted = sort_list(topics, key=1)\n",
    "    for i, topics in enumerate(topics_sorted[:k]):\n",
    "        idx = topics[0]\n",
    "        print (i, all_topics[idx])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, '0.064*\"wife\" + 0.064*\"focus\" + 0.064*\"driving\" + 0.064*\"depressed\" + 0.064*\"hard\" + 0.064*\"go\" + 0.064*\"road\" + 0.064*\"staying\" + 0.064*\"likes\" + 0.064*\"home\"')\n",
      "1 (8, '0.064*\"home\" + 0.064*\"prefer\" + 0.064*\"football\" + 0.064*\"usually\" + 0.064*\"watching\" + 0.064*\"reading\" + 0.064*\"tv\" + 0.064*\"prefers\" + 0.064*\"wife\" + 0.064*\"playing\"')\n"
     ]
    }
   ],
   "source": [
    "topics= model.get_document_topics( doc_bow )\n",
    "top_k_topics = print_top_k(topics, all_topics, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top predictions for the new sentence `My wife plans to go out tonight.` are printed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['going', 'play', 'soccer', 'kids']\n",
      "[(19, 1)]\n",
      "[0.08770634 0.1110798  0.9765234  0.11107814 0.97384095]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "lda_index = similarities.MatrixSimilarity(model[numerical_corpus])\n",
    "\n",
    "doc_new = \"We are going play soccer with the kids\"\n",
    "doc_new_prepared = prepare_text(doc_new, TOKENIZE=True)\n",
    "print ( doc_new_prepared )\n",
    "doc_bow = dictionary.doc2bow(doc_new_prepared)\n",
    "print (doc_bow)\n",
    "\n",
    "similarities = lda_index[model[doc_bow]]\n",
    "\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means this new sentence is closest in the meaning to `doc3`  with probability `0.976`. And it makes sense that the new doc `We are going play soccer with the kids` is closest in meaning to `Kids are playing football in the field and they seem to have fun`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things that can be added here\n",
    "\n",
    "* N grams vocabulary\n",
    "* Word Embeddings where `play` and `playing` mean the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
